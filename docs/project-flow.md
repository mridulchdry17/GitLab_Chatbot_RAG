# Complete Project Flow: After Data Collection

## Overview

This document explains the complete flow of the GitLab Handbook Chatbot from data collection to user interaction.

---

## Phase 1: Data Collection (DONE âœ…)

### Step 1: Scraping
```
python src/scraper.py
```

**What happens:**
1. Scraper visits GitLab Handbook & Direction pages
2. Recursively follows links (depth 3, max 100 pages)
3. Extracts content with metadata:
   - `source_url`
   - `section_title`
   - `start_char`
   - `end_char`
   - `content`

**Output:** `data/gitlab_chunks.json` (22,290 lines)

---

## Phase 2: Initialization (When App Starts)

### Step 2: User Clicks "Initialize System"

**Location:** `app.py` â†’ `initialize_system()`

**Flow:**
```
1. Create VectorStore instance
   â†“
2. VectorStore.initialize()
   â†“
3. Load chunks from gitlab_chunks.json
   â†“
4. Convert to LangChain Documents (with metadata)
   â†“
5. Apply token-based chunking (~300 tokens, 50 overlap)
   â†“
6. Create embeddings (HuggingFace)
   â†“
7. Store in ChromaDB
   â†“
8. Create Chatbot instance
   â†“
9. Initialize Analytics
   â†“
10. System Ready! âœ…
```

### Detailed Steps:

#### A. Vector Store Initialization (`vector_store.py`)

```python
# 1. Load chunks from JSON
chunks = load_chunks()  # Reads gitlab_chunks.json

# 2. Convert to LangChain Documents
documents = _create_langchain_documents(chunks)
# Each document has: page_content + metadata

# 3. Token-based chunking (NEW!)
documents = _split_with_metadata_preservation(documents)
# Splits large sections into ~300 token chunks
# Preserves metadata (source_url, start_char, end_char)

# 4. Create embeddings
embeddings = HuggingFaceEmbeddings('all-MiniLM-L6-v2')
# Converts text â†’ 384-dimensional vectors

# 5. Store in ChromaDB
vector_store.add_documents(documents)
# Saves: embeddings + documents + metadata
```

**Result:** Vector database ready for semantic search

---

## Phase 3: User Interaction (When User Asks Question)

### Step 3: User Types Question

**Example:** "What is GitLab's approach to transparency?"

**Flow:**
```
User Question
    â†“
app.py (chat input)
    â†“
chatbot.generate_response(query)
    â†“
[Multiple steps happen here]
    â†“
Response displayed to user
```

### Detailed Flow Inside `generate_response()`:

#### Step 3.1: Guardrail Check
```python
check_query_appropriateness(query)
# Checks for inappropriate keywords
# Returns: (is_appropriate, message)
```

**If blocked:** Returns guardrail message, stops here

**If allowed:** Continues to next step

---

#### Step 3.2: Vector Search (RAG Retrieval)

```python
# Search for relevant chunks
search_results = vector_store.search(query, n_results=5)
```

**What happens:**
1. Convert query to embedding (same model)
2. Search ChromaDB for similar vectors
3. Returns top 5 most similar chunks
4. Each result includes:
   - `content` (text)
   - `source_url`
   - `section_title`
   - `start_char`, `end_char`
   - `distance` (similarity score)

**Result:** 5 relevant chunks from Handbook

---

#### Step 3.3: Get Source Documents (LangChain)

```python
source_documents = retriever.get_relevant_documents(query)
# Gets LangChain Document objects with metadata
```

**Purpose:** For source citations

---

#### Step 3.4: Format Context

```python
# Format documents into context string
context = format_docs(source_documents)
# Combines all chunks into one text
```

**Example output:**
```
[Source 1]
Section: Transparency
URL: https://handbook.gitlab.com/values
Content: GitLab values transparency...

[Source 2]
Section: Open Communication
URL: https://handbook.gitlab.com/values
Content: We believe in open communication...
```

---

#### Step 3.5: Build Prompt

```python
formatted_prompt = prompt_template.format(
    context=context,           # Retrieved chunks
    question=query,            # User's question
    chat_history=chat_history_str  # Previous conversation
)
```

**Prompt includes:**
- System instructions (guardrails)
- Previous conversation (if any)
- Retrieved context from Handbook
- User's question

---

#### Step 3.6: Generate Response (LLM)

```python
response = llm.invoke(formatted_prompt)
# Sends to Google Gemini API
# Returns: AI-generated answer
```

**What Gemini does:**
- Reads the context
- Understands the question
- Generates answer based on Handbook content
- Includes source citations

---

#### Step 3.7: Extract Sources & Metadata

```python
sources = extract_sources(source_documents, search_results)
# Extracts: url, section_title, relevance_score
```

**For transparency:** User can see which pages were used

---

#### Step 3.8: Calculate Confidence

```python
# Based on similarity scores
avg_distance = average of search result distances
if avg_distance < 0.3: confidence = 'high'
elif avg_distance < 0.5: confidence = 'medium'
else: confidence = 'low'
```

**Purpose:** Show user how confident the system is

---

#### Step 3.9: Track Analytics

```python
analytics.track_query(query, response)
# Saves: query, confidence, sources, timestamp
```

**Purpose:** Usage insights

---

#### Step 3.10: Return Response

```python
return {
    'response': "GitLab values transparency...",
    'sources': [...],
    'confidence': 'high',
    'context_used': True
}
```

---

## Phase 4: Display to User

### Step 4: Show in UI (`app.py`)

**What user sees:**
1. **Answer** - The generated response
2. **Sources** - Clickable links to Handbook pages
3. **Confidence Badge** - ðŸŸ¢ High / ðŸŸ¡ Medium / ðŸ”´ Low
4. **Context Preview** - (Optional) Shows which sources will be used

**Features:**
- Conversation history maintained
- Can ask follow-up questions
- Export conversation
- View analytics

---

## Complete Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. DATA COLLECTION (scraper.py)                         â”‚
â”‚    Scrape â†’ Extract â†’ Save to JSON                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. INITIALIZATION (app.py â†’ initialize_system)          â”‚
â”‚    Load JSON â†’ Token Chunking â†’ Embeddings â†’ ChromaDB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. USER ASKS QUESTION                                    â”‚
â”‚    "What is transparency?"                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. GUARDRAIL CHECK (chatbot.py)                          â”‚
â”‚    Is query appropriate? â†’ Yes/No                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“ (if Yes)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. VECTOR SEARCH (vector_store.py)                       â”‚
â”‚    Query â†’ Embedding â†’ Search ChromaDB â†’ Top 5 chunks   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. FORMAT CONTEXT (chatbot.py)                           â”‚
â”‚    Combine chunks â†’ Format with metadata                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. BUILD PROMPT (chatbot.py)                             â”‚
â”‚    System prompt + Context + Question + History         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. LLM GENERATION (Gemini API)                            â”‚
â”‚    Send prompt â†’ Generate answer                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. POST-PROCESSING (chatbot.py)                          â”‚
â”‚    Extract sources â†’ Calculate confidence â†’ Track        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10. DISPLAY (app.py)                                     â”‚
â”‚     Show answer + sources + confidence + analytics       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Components & Their Roles

### 1. Vector Store (`vector_store.py`)
- **Purpose:** Semantic search engine
- **Does:** Stores embeddings, searches for similar content
- **Uses:** LangChain ChromaDB, HuggingFace embeddings

### 2. Chatbot (`chatbot.py`)
- **Purpose:** RAG orchestration
- **Does:** Retrieves context, formats prompts, calls LLM
- **Uses:** LangChain, Google Gemini API

### 3. Analytics (`analytics.py`)
- **Purpose:** Usage tracking
- **Does:** Records queries, sources, confidence
- **Uses:** JSON file storage

### 4. App (`app.py`)
- **Purpose:** User interface
- **Does:** Displays UI, handles user input
- **Uses:** Streamlit

---

## Data Flow Summary

```
JSON File (gitlab_chunks.json)
    â†“
LangChain Documents (with metadata)
    â†“
Token-based Chunks (~300 tokens)
    â†“
Embeddings (384-dim vectors)
    â†“
ChromaDB (vector database)
    â†“
[User Query]
    â†“
Query Embedding
    â†“
Similarity Search
    â†“
Top 5 Chunks (with metadata)
    â†“
Context for LLM
    â†“
Gemini API
    â†“
Answer + Sources
    â†“
Display to User
```

---

## What Makes This RAG (Retrieval-Augmented Generation)?

1. **Retrieval:** Vector search finds relevant Handbook content
2. **Augmentation:** Context added to prompt
3. **Generation:** LLM generates answer based on retrieved context

**Result:** Answers are grounded in actual Handbook content, not just LLM knowledge!

---

## Current Status

âœ… **Data Collection:** Complete (22,290 chunks)
â­ï¸ **Next:** Initialize system (creates embeddings)
â­ï¸ **Then:** Ready for questions!

